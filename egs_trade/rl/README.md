&emsp;&emsp;自从2017年AlphaGo与柯洁围棋大战之后，深度强化学习大火。

&emsp;&emsp;相比于机器学习和深度学习, 强化学习是以最终目标为导向 (以交互作为目标) , 
而很多其他方法是考虑孤立的子问题 (如“股价预测”,“大盘预测”,“交易决策”等) , 这并不能直接获得交互的动作, 
比如“命令机器人炒股盈利”, 这个任务包含了“股价预测”,”大盘预测”等等, 而强化学习的目标则是“完成命令者的任务”, 
可以直接得到“炒股盈利”的一连贯动作。 

![trades_on_k_line](.README_images/强化学习.png)

1. 样例介绍：

    | **序号**    | **策略**  | **代码路径** |  **论文** |   
    |:-------- |:-------- |:-------| :-------| 
    | 1 | 原型  | egs_trade/rl/a001_proto_sb3 |  |
    | 2 | 多股票交易 | egs_trade/rl/a002_finRL/a01_Stock_NeurIPS2018 | Practical Deep Reinforcement Learning Approach for Stock Trading （https://arxiv.org/abs/1811.07522） |

2. 样例回测详情

    | **序号**    | **策略**  | **市场**  | **年化收益** |  **最大回撤** |  **夏普率** | 
    |:-------- |:-------- |:-------| :-------| :-------| :-------| 
    | 1 | 原型 | 中国A股 |  |  | | 
    | 2 | 多股票交易 | 美股道儿琼斯30 | 53.1% | -10.4%  | 2.17 |
